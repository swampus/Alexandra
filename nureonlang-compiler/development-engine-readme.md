# Alexandra Neural Development Engine

Alexandra supports a **development-phase compilation model** in which neural networks are not always defined explicitly.
Instead, they can be generated from a compact **genetic program (genome)** written in NureonLang.

This enables deterministic expansion of neural architectures, compact storage of large network definitions, and opens the door for evolutionary or generative approaches to architecture design.

---

## ğŸ§¬ Concept: Genome â†’ Development â†’ Network

Traditional pipeline:

```
Source â†’ IR â†’ Compiler â†’ Network
```

Development-enabled pipeline:

```
Source (Genome)
        â†“
IR (Genetic Program)
        â†“
Development Engine
        â†“
Expanded IR (Concrete Instructions)
        â†“
Compiler
        â†“
NetworkModel
```

In this mode, loops, conditions, macros, and expansion rules are resolved **before** network compilation.

---

## âœ¨ Example

Instead of writing:

```
LAYER dense1 size=32
LAYER dense2 size=32
LAYER dense3 size=32
```

You can write:

```
FOR i FROM 1 TO 3
BEGIN
    LAYER dense[i] size=32
END
```

During development, this expands to:

```
LAYER dense1 size=32
LAYER dense2 size=32
LAYER dense3 size=32
```

The compiler then builds the network from the expanded instructions.

---

## ğŸš€ Why Development Mode Exists

### 1. Compact architecture definition

Large networks can be described in a few lines of code.

### 2. Deterministic generation

The same genome always produces the same network.

### 3. Foundation for neural evolution

Genetic programs can be mutated or recombined.

### 4. Easier cluster / swarm definitions

Multiple agents or networks can be generated programmatically.

### 5. Separation of concerns

* Development Engine = expands structure
* Compiler = builds runtime model

---

## ğŸ› ï¸ What the Development Engine Does

The development stage currently supports:

* **FOR loops** â†’ unrolled into repeated instructions
* **IF statements** â†’ resolved into a single branch
* **Macros / modules** â†’ inlined into the instruction stream
* **Nested structures** â†’ expanded recursively

The result is always a **flat IR without generative constructs**.

---

## ğŸ”§ Enabling Development Mode

Development mode can be enabled in the compilation pipeline.

Example:

```java
Instruction genome = parser.parse(source);

IRDeveloper developer = new IRDeveloper(List.of(
        new ForExpander(),
        new IfExpander()
));

List<Instruction> developed = developer.develop(List.of(genome));

NetworkModel model = compiler.compile(developed);
```

If development mode is disabled, the compiler works directly on the original IR.

---

## ğŸ§ª Testing

Unit tests cover:

* loop expansion correctness
* empty body handling
* boundary conditions
* integration of development pipeline

Run tests:

```
mvn test
```

---

## ğŸ§  Design Philosophy

Alexandra treats neural architecture descriptions not only as static topologies, but also as **developmental programs**.

This mirrors biological systems:

| Biology     | Alexandra             |
| ----------- | --------------------- |
| DNA         | NureonLang program    |
| Development | IRDeveloper expansion |
| Organism    | NetworkModel          |

The network is not stored explicitly â€” it is **developed from a compact genome**.

---

## ğŸ“Œ Future Extensions

Planned improvements include:

* expression evaluation engine for conditions
* mutation operators for genome evolution
* caching of developed IR
* visualization of development stages
* probabilistic or stochastic expansion rules

---

## ğŸ“œ License

(put your project license here)

---

## ğŸ‘¤ Author

Swampus / Alexandra Project
